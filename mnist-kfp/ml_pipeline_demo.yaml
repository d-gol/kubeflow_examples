apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: ml-first-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.4.1, pipelines.kubeflow.org/pipeline_compilation_time: '2021-11-12T10:48:04.029132',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "ML first).", "name": "ML
      first"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.4.1}
spec:
  entrypoint: ml-first
  templates:
  - name: ml-first
    dag:
      tasks:
      - name: model-fair
        template: model-fair
        dependencies: [preprocess-data]
        arguments:
          artifacts:
          - {name: preprocess-data-output_text, from: '{{tasks.preprocess-data.outputs.artifacts.preprocess-data-output_text}}'}
      - name: model-full
        template: model-full
        dependencies: [preprocess-data]
        arguments:
          artifacts:
          - {name: preprocess-data-output_text, from: '{{tasks.preprocess-data.outputs.artifacts.preprocess-data-output_text}}'}
      - name: models-evaluate
        template: models-evaluate
        dependencies: [model-fair, model-full]
        arguments:
          artifacts:
          - {name: model-fair-output_text, from: '{{tasks.model-fair.outputs.artifacts.model-fair-output_text}}'}
          - {name: model-full-output_text, from: '{{tasks.model-full.outputs.artifacts.model-full-output_text}}'}
      - name: preprocess-data
        template: preprocess-data
        dependencies: [read-data]
        arguments:
          artifacts:
          - {name: read-data-output_text, from: '{{tasks.read-data.outputs.artifacts.read-data-output_text}}'}
      - {name: read-data, template: read-data}
  - name: model-fair
    container:
      args: [--text, /tmp/inputs/text/data, --output-text, /tmp/outputs/output_text/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def model_fair(text_path, output_text_path):
            import tensorflow as tf
            from zipfile import ZipFile
            import numpy as np

            model = tf.keras.Sequential()
            model.add(tf.keras.layers.Flatten(input_shape=(28,28,1)))
            model.add(tf.keras.layers.Dense(2, activation='relu'))
            model.add(tf.keras.layers.Dense(1))

            model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
                          optimizer=tf.keras.optimizers.Adam(),
                          metrics=['accuracy'])

            model.summary()

            with ZipFile(text_path, 'r') as zipObj:
               zipObj.extractall()

            # Load data
            x_train = np.load('xtrain_filtered.npy')
            y_train = np.load('ytrain_filtered.npy')

            x_test = np.load('xtest_filtered.npy')
            y_test = np.load('ytest_filtered.npy')

            model.fit(x_train, y_train, batch_size=128, epochs=1, verbose=1, validation_data=(x_test, y_test))

            cnn_results = model.evaluate(x_test, y_test)

            with open(output_text_path, 'w') as writer:
                writer.write(str(cnn_results) + '\n')

        import argparse
        _parser = argparse.ArgumentParser(prog='Model fair', description='')
        _parser.add_argument("--text", dest="text_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--output-text", dest="output_text_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = model_fair(**_parsed_args)
      image: python:3.7
    inputs:
      artifacts:
      - {name: preprocess-data-output_text, path: /tmp/inputs/text/data}
    outputs:
      artifacts:
      - {name: model-fair-output_text, path: /tmp/outputs/output_text/data}
    metadata:
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--text", {"inputPath": "text"}, "--output-text", {"outputPath":
          "output_text"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef model_fair(text_path, output_text_path):\n    import
          tensorflow as tf\n    from zipfile import ZipFile\n    import numpy as np\n\n    model
          = tf.keras.Sequential()\n    model.add(tf.keras.layers.Flatten(input_shape=(28,28,1)))\n    model.add(tf.keras.layers.Dense(2,
          activation=''relu''))\n    model.add(tf.keras.layers.Dense(1))\n\n    model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n                  optimizer=tf.keras.optimizers.Adam(),\n                  metrics=[''accuracy''])\n\n    model.summary()\n\n    with
          ZipFile(text_path, ''r'') as zipObj:\n       zipObj.extractall()\n\n    #
          Load data\n    x_train = np.load(''xtrain_filtered.npy'')\n    y_train =
          np.load(''ytrain_filtered.npy'')\n\n    x_test = np.load(''xtest_filtered.npy'')\n    y_test
          = np.load(''ytest_filtered.npy'')\n\n    model.fit(x_train, y_train, batch_size=128,
          epochs=1, verbose=1, validation_data=(x_test, y_test))\n\n    cnn_results
          = model.evaluate(x_test, y_test)\n\n    with open(output_text_path, ''w'')
          as writer:\n        writer.write(str(cnn_results) + ''\\n'')\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Model fair'', description='''')\n_parser.add_argument(\"--text\",
          dest=\"text_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-text\",
          dest=\"output_text_path\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = model_fair(**_parsed_args)\n"], "image": "python:3.7"}}, "inputs": [{"name":
          "text"}], "name": "Model fair", "outputs": [{"name": "output_text"}]}',
        pipelines.kubeflow.org/component_ref: '{}'}
  - name: model-full
    container:
      args: [--text, /tmp/inputs/text/data, --output-text, /tmp/outputs/output_text/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def model_full(text_path, output_text_path):
            # A simple model based off LeNet from https://keras.io/examples/mnist_cnn/
            import tensorflow as tf
            from zipfile import ZipFile
            import numpy as np

            model = tf.keras.Sequential()
            model.add(tf.keras.layers.Conv2D(32, [3, 3], activation='relu', input_shape=(28,28,1)))
            model.add(tf.keras.layers.Conv2D(64, [3, 3], activation='relu'))
            model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))
            model.add(tf.keras.layers.Dropout(0.25))
            model.add(tf.keras.layers.Flatten())
            model.add(tf.keras.layers.Dense(128, activation='relu'))
            model.add(tf.keras.layers.Dropout(0.5))
            model.add(tf.keras.layers.Dense(1))

            model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
                          optimizer=tf.keras.optimizers.Adam(),
                          metrics=['accuracy'])

            model.summary()

            with ZipFile(text_path, 'r') as zipObj:
               zipObj.extractall()

            # Load data
            x_train = np.load('xtrain_filtered.npy')
            y_train = np.load('ytrain_filtered.npy')

            x_test = np.load('xtest_filtered.npy')
            y_test = np.load('ytest_filtered.npy')

            model.fit(x_train, y_train, batch_size=128, epochs=1, verbose=1, validation_data=(x_test, y_test))

            cnn_results = model.evaluate(x_test, y_test)

            with open(output_text_path, 'w') as writer:
                writer.write(str(cnn_results) + '\n')

        import argparse
        _parser = argparse.ArgumentParser(prog='Model full', description='')
        _parser.add_argument("--text", dest="text_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--output-text", dest="output_text_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = model_full(**_parsed_args)
      image: python:3.7
    inputs:
      artifacts:
      - {name: preprocess-data-output_text, path: /tmp/inputs/text/data}
    outputs:
      artifacts:
      - {name: model-full-output_text, path: /tmp/outputs/output_text/data}
    metadata:
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--text", {"inputPath": "text"}, "--output-text", {"outputPath":
          "output_text"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef model_full(text_path, output_text_path):\n    #
          A simple model based off LeNet from https://keras.io/examples/mnist_cnn/\n    import
          tensorflow as tf\n    from zipfile import ZipFile\n    import numpy as np\n\n    model
          = tf.keras.Sequential()\n    model.add(tf.keras.layers.Conv2D(32, [3, 3],
          activation=''relu'', input_shape=(28,28,1)))\n    model.add(tf.keras.layers.Conv2D(64,
          [3, 3], activation=''relu''))\n    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,
          2)))\n    model.add(tf.keras.layers.Dropout(0.25))\n    model.add(tf.keras.layers.Flatten())\n    model.add(tf.keras.layers.Dense(128,
          activation=''relu''))\n    model.add(tf.keras.layers.Dropout(0.5))\n    model.add(tf.keras.layers.Dense(1))\n\n    model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n                  optimizer=tf.keras.optimizers.Adam(),\n                  metrics=[''accuracy''])\n\n    model.summary()\n\n    with
          ZipFile(text_path, ''r'') as zipObj:\n       zipObj.extractall()\n\n    #
          Load data\n    x_train = np.load(''xtrain_filtered.npy'')\n    y_train =
          np.load(''ytrain_filtered.npy'')\n\n    x_test = np.load(''xtest_filtered.npy'')\n    y_test
          = np.load(''ytest_filtered.npy'')\n\n    model.fit(x_train, y_train, batch_size=128,
          epochs=1, verbose=1, validation_data=(x_test, y_test))\n\n    cnn_results
          = model.evaluate(x_test, y_test)\n\n    with open(output_text_path, ''w'')
          as writer:\n        writer.write(str(cnn_results) + ''\\n'')\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Model full'', description='''')\n_parser.add_argument(\"--text\",
          dest=\"text_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-text\",
          dest=\"output_text_path\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = model_full(**_parsed_args)\n"], "image": "python:3.7"}}, "inputs": [{"name":
          "text"}], "name": "Model full", "outputs": [{"name": "output_text"}]}',
        pipelines.kubeflow.org/component_ref: '{}'}
  - name: models-evaluate
    container:
      args: [--text-path-0, /tmp/inputs/text_path_0/data, --text-path-1, /tmp/inputs/text_path_1/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def models_evaluate(text_path_0, text_path_1):
            print('model 0:')
            with open(text_path_0, 'r') as reader:
                for line in reader:
                    print(line, end = '')
            print('model 1:')
            with open(text_path_1, 'r') as reader:
                for line in reader:
                    print(line, end = '')

        import argparse
        _parser = argparse.ArgumentParser(prog='Models evaluate', description='')
        _parser.add_argument("--text-path-0", dest="text_path_0", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--text-path-1", dest="text_path_1", type=str, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = models_evaluate(**_parsed_args)
      image: python:3.7
    inputs:
      artifacts:
      - {name: model-full-output_text, path: /tmp/inputs/text_path_0/data}
      - {name: model-fair-output_text, path: /tmp/inputs/text_path_1/data}
    metadata:
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--text-path-0", {"inputPath": "text_path_0"}, "--text-path-1",
          {"inputPath": "text_path_1"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def models_evaluate(text_path_0, text_path_1):\n    print(''model 0:'')\n    with
          open(text_path_0, ''r'') as reader:\n        for line in reader:\n            print(line,
          end = '''')\n    print(''model 1:'')\n    with open(text_path_1, ''r'')
          as reader:\n        for line in reader:\n            print(line, end = '''')\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Models evaluate'', description='''')\n_parser.add_argument(\"--text-path-0\",
          dest=\"text_path_0\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--text-path-1\",
          dest=\"text_path_1\", type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = models_evaluate(**_parsed_args)\n"],
          "image": "python:3.7"}}, "inputs": [{"name": "text_path_0"}, {"name": "text_path_1"}],
          "name": "Models evaluate"}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: preprocess-data
    container:
      args: [--text, /tmp/inputs/text/data, --output-text, /tmp/outputs/output_text/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def preprocess_data(text_path, output_text_path):
            import numpy as np
            import os
            import tarfile
            print('tarfile imported')
            from zipfile import ZipFile

            with ZipFile(text_path, 'r') as zipObj:
               zipObj.extractall()

            # Load data
            x_train = np.load('xtrain.npy')
            y_train = np.load('ytrain.npy')

            x_test = np.load('xtest.npy')
            y_test = np.load('ytest.npy')

            # Filter 3 and 6
            def filter_36(x, y):
                keep = (y == 3) | (y == 6)
                x, y = x[keep], y[keep]
                y = y == 3
                return x,y

            print("Number of unfiltered training examples:", len(x_train))
            print("Number of unfiltered test examples:", len(x_test))

            x_train, y_train = filter_36(x_train, y_train)
            x_test, y_test = filter_36(x_test, y_test)

            print("Number of filtered training examples:", len(x_train))
            print("Number of filtered test examples:", len(x_test))

            # Save modified data
            np.save('xtrain_filtered.npy', x_train)
            np.save('ytrain_filtered.npy', y_train)

            np.save('xtest_filtered.npy', x_test)
            np.save('ytest_filtered.npy', y_test)

            zipObj = ZipFile(output_text_path, 'w')

            zipObj.write('xtrain_filtered.npy')
            zipObj.write('ytrain_filtered.npy')
            zipObj.write('xtest_filtered.npy')
            zipObj.write('ytest_filtered.npy')

            zipObj.close()

        import argparse
        _parser = argparse.ArgumentParser(prog='Preprocess data', description='')
        _parser.add_argument("--text", dest="text_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--output-text", dest="output_text_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = preprocess_data(**_parsed_args)
      image: python:3.7
    inputs:
      artifacts:
      - {name: read-data-output_text, path: /tmp/inputs/text/data}
    outputs:
      artifacts:
      - {name: preprocess-data-output_text, path: /tmp/outputs/output_text/data}
    metadata:
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--text", {"inputPath": "text"}, "--output-text", {"outputPath":
          "output_text"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef preprocess_data(text_path, output_text_path):\n    import
          numpy as np\n    import os\n    import tarfile\n    print(''tarfile imported'')\n    from
          zipfile import ZipFile\n\n    with ZipFile(text_path, ''r'') as zipObj:\n       zipObj.extractall()\n\n    #
          Load data\n    x_train = np.load(''xtrain.npy'')\n    y_train = np.load(''ytrain.npy'')\n\n    x_test
          = np.load(''xtest.npy'')\n    y_test = np.load(''ytest.npy'')\n\n    # Filter
          3 and 6\n    def filter_36(x, y):\n        keep = (y == 3) | (y == 6)\n        x,
          y = x[keep], y[keep]\n        y = y == 3\n        return x,y\n\n    print(\"Number
          of unfiltered training examples:\", len(x_train))\n    print(\"Number of
          unfiltered test examples:\", len(x_test))\n\n    x_train, y_train = filter_36(x_train,
          y_train)\n    x_test, y_test = filter_36(x_test, y_test)\n\n    print(\"Number
          of filtered training examples:\", len(x_train))\n    print(\"Number of filtered
          test examples:\", len(x_test))\n\n    # Save modified data\n    np.save(''xtrain_filtered.npy'',
          x_train)\n    np.save(''ytrain_filtered.npy'', y_train)\n\n    np.save(''xtest_filtered.npy'',
          x_test)\n    np.save(''ytest_filtered.npy'', y_test)\n\n    zipObj = ZipFile(output_text_path,
          ''w'')\n\n    zipObj.write(''xtrain_filtered.npy'')\n    zipObj.write(''ytrain_filtered.npy'')\n    zipObj.write(''xtest_filtered.npy'')\n    zipObj.write(''ytest_filtered.npy'')\n\n    zipObj.close()\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Preprocess data'', description='''')\n_parser.add_argument(\"--text\",
          dest=\"text_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-text\",
          dest=\"output_text_path\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = preprocess_data(**_parsed_args)\n"], "image": "python:3.7"}}, "inputs":
          [{"name": "text"}], "name": "Preprocess data", "outputs": [{"name": "output_text"}]}',
        pipelines.kubeflow.org/component_ref: '{}'}
  - name: read-data
    container:
      args: [--output-text, /tmp/outputs/output_text/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def read_data(output_text_path):
            import tensorflow as tf
            import numpy as np
            import os
            from zipfile import ZipFile

            (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

            # Rescale the images from [0,255] to the [0.0,1.0] range.
            x_train, x_test = x_train[..., np.newaxis]/255.0, x_test[..., np.newaxis]/255.0

            np.save('xtrain.npy', x_train)
            np.save('ytrain.npy', y_train)

            np.save('xtest.npy', x_test)
            np.save('ytest.npy', y_test)

            zipObj = ZipFile(output_text_path, 'w')

            zipObj.write('xtrain.npy')
            zipObj.write('ytrain.npy')
            zipObj.write('xtest.npy')
            zipObj.write('ytest.npy')

            zipObj.close()

        import argparse
        _parser = argparse.ArgumentParser(prog='Read data', description='')
        _parser.add_argument("--output-text", dest="output_text_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = read_data(**_parsed_args)
      image: python:3.7
    outputs:
      artifacts:
      - {name: read-data-output_text, path: /tmp/outputs/output_text/data}
    metadata:
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--output-text", {"outputPath": "output_text"}], "command": ["sh",
          "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef read_data(output_text_path):\n    import tensorflow as
          tf\n    import numpy as np\n    import os\n    from zipfile import ZipFile\n\n    (x_train,
          y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n\n    #
          Rescale the images from [0,255] to the [0.0,1.0] range.\n    x_train, x_test
          = x_train[..., np.newaxis]/255.0, x_test[..., np.newaxis]/255.0\n\n    np.save(''xtrain.npy'',
          x_train)\n    np.save(''ytrain.npy'', y_train)\n\n    np.save(''xtest.npy'',
          x_test)\n    np.save(''ytest.npy'', y_test)\n\n    zipObj = ZipFile(output_text_path,
          ''w'')\n\n    zipObj.write(''xtrain.npy'')\n    zipObj.write(''ytrain.npy'')\n    zipObj.write(''xtest.npy'')\n    zipObj.write(''ytest.npy'')\n\n    zipObj.close()\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Read data'', description='''')\n_parser.add_argument(\"--output-text\",
          dest=\"output_text_path\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = read_data(**_parsed_args)\n"], "image": "python:3.7"}}, "name": "Read
          data", "outputs": [{"name": "output_text", "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{}'}
  arguments:
    parameters: []
  serviceAccountName: pipeline-runner
